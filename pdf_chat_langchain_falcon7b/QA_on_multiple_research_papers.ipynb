{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0e01dd-7257-4268-bded-b599694060c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c755c6c9-8ece-49c6-b69f-bf66aa57584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8f88f5-081a-45be-a729-3e49469b5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings \n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e594a1-1f90-4915-a065-f806fcdf0990",
   "metadata": {},
   "source": [
    "### Loading LLM -> tiiuae/falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62410ad6-8c83-4da8-a19d-d5640eebeba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 19:05:21.258270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 19:05:21.764967: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-12 19:05:21.765012: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-12 19:05:21.765016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/srivatsa/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/srivatsa/anaconda3/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /home/srivatsa/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e25ab1547f043419a26277e1a813ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(repo_id, load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    max_new_tokens = 300,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temprature\": 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5393017-9f7a-459f-8bdd-824cda7efff2",
   "metadata": {},
   "source": [
    "### Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0806e8-6117-4dae-9e00-0eefad4098fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952ea8a-d824-4bcd-ae30-7f3db023549c",
   "metadata": {},
   "source": [
    "### Creating Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c69c86-7e8b-4093-ad7b-3f5c814ed02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vector_db = FAISS.load_local(\"faiss_index\", embedding)\n",
    "except FileIOReader as err:\n",
    "    loader = DirectoryLoader(\"./research_papers/\", glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "    research_documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(research_documents)\n",
    "    print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae13b13-cf69-4dce-9fbd-46bc16c23a03",
   "metadata": {},
   "source": [
    "### Creating DB from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da4d905-6c42-4a22-822f-437bc752b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# vector_db = FAISS.from_documents(\n",
    "#     documents=texts,\n",
    "#     embedding=embedding\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53a55d1-4f2f-47f7-8ac6-70c222de3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "try:\n",
    "    vector_db = FAISS.load_local(\"faiss_index\", embedding)\n",
    "except Exception as exp:\n",
    "    vector_db = FAISS.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embedding\n",
    "    )\n",
    "    vector_db.save_local(\"faiss_index\")\n",
    "    vector_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c73ffa-a15d-42c8-93e6-866e2326570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_db.similarity_search(\"what is self attention ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81066857-d590-4cc7-9139-4fea9ec3d26d",
   "metadata": {},
   "source": [
    "### Creating LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95cd7cca-aece-4135-9f56-0fd66f814305",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = load_qa_chain(llm=llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d3799e-a95d-49e8-a21a-acd0abc873f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query):    \n",
    "    answer = qa_chain.run(\n",
    "        input_documents=vector_db.similarity_search(query),\n",
    "        question=query\n",
    "    )\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc9a69d-3315-4860-824c-ff519af3399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Both `max_new_tokens` (=300) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A transformer is a type of ML model that typically converts the input text to a numerical value and outputs a representation of the text as a numerical value. These text-to-numeric models are popular in NLP tasks such as language translation and summarization.\n",
      "\n",
      "The Transformer model is a popular NLP model that is used to generate text. It is able to generate text from previous texts and can be used to answer questions asked on the Transformer model. It is often used for summarization and translation tasks.\n",
      "The Transformer model is a type of Neural Network used to answer language-related questions such as translation, summarization, or classification. The Transformer model is composed of three parts: a data model, an encoding model and a decoding model.\n",
      "A common type of text summarization model.\n",
      "The transformer model is used to generate text with or without any training dataset. It is able to generate text with many different structures and can be used in natural language processing tasks. The transformer model is commonly used to generate articles for search engine optimization tasks such as content creation.\n",
      "A transformer is a type of Neural Network that can answer a wide range of language-related questions such as translation, summarization, and classification. The Transformer model typically consists of a data model, an encoding model and a decoding model.\n",
      "The Transformer model is a type of neural network used to generate text from previous texts. It can be used to answer a variety of language-related questions such as\n"
     ]
    }
   ],
   "source": [
    "query = \"what is a transformer?\"\n",
    "answer_question(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5eca87e-b81b-4af7-883c-d430967c28b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Both `max_new_tokens` (=300) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A transformer is a multi-layered neural network with one encoder layer and multiple decoder layers. Instead of using the traditional self-attention of neural networks, transformers use position-based attention as well as context representation to encode and decode sequences. So, while recurrent neural networks may use the same input sequence to generate sequence-to-sequence tasks, transformers use the position of the elements in the input sequence to generate translation tasks.\n"
     ]
    }
   ],
   "source": [
    "query = \"How is a transformer different from recurrent neural networks ?\"\n",
    "answer_question(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327229ea-ef05-4656-9904-3186385fcb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Both `max_new_tokens` (=300) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Transformer network is composed of several layers of neural networks, each stacked\n",
      "on top of each other to recreate the functionality of a transformer. The number and types of layers\n",
      "in the network depend on its desired functionality, the number of tasks it will be handling\n",
      "and, most importantly, its accuracy. They are divided into (1) Encoder, (2) Decoder, (3) Encoder-Decoder (E-D),\n",
      "Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs). The main difference\n",
      "between the last two (the convolutional neural networks and recurrent neural networks) is that\n",
      "CNNs can use variable-sized input data and use more computing power than fixed 2D trans-\n",
      "formers, while the former can be used with a very wide range of input sizes, with no\n",
      "computing power penalty and with little information loss.\n",
      "4 Encoder: A single-layer (2-dimensional) neural network that performs each of the basic computa-\n",
      "tional layers. This layer is used to generate the low-level features of the model, while other layers\n",
      "focus on higher level features and combining these features to generate even more lower-level\n",
      "features. Encoder is the most complex layer of the Transformer network. Depending on the de-\n",
      "veloped model, some number of encoder layers can be stacked together to reproduce a\n",
      "transformer\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the different kinds of transformer networks ? Can you explain each of them ?\"\n",
    "answer_question(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fee4044-fea8-4091-b9b4-82b61d88ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# effnet_b7 = None\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a3f11-0fdb-4625-9aa5-0140de34abe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
